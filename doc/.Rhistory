ylab = "Proportion of Variance Explained",
type = "b")
eigen(R)
prin_comp=prcomp(track.new, scale. = T)
std_dev = prin_comp$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
plot(prop_varex, xlab = "Principal Component",
ylab = "Proportion of Variance Explained",
type = "b")
diag(L)
cumsum(diag(L))/sum(diag(L))
diag(L)/sum(diag(L))  # proportions of variance explained
prin_comp
R
L <- diag(eigen(R)$values)
L <- diag(eigen(R)$values); L
L <- diag(eigen(R)$values);
L <- diag(eigen(R)$values); L
L <- diag(eigen(R)$values); L
eigen(R)$values
eigen(R)$values  # eigenvalues and eigenvetors
eigen(R)$vectors
track[,1] * sqrt(eigen(R)$values)
eigen(R)$vectors
eigen(R)$vectors[,1] * sqrt(eigen(R)$values)
eigen(R)$values[1]
eigen(R)$vectors[,1] * sqrt(eigen(R)$values[1])
as.matrix(scale(track)) %*% as.matrix(eigen(R[,1]))
eigen(R[,1])
as.matrix(scale(track)) %*% as.matrix(eigen(R)$vectors[,1]))
eigen(R)$vectors[,1]
as.matrix(scale(track)) %*% as.matrix(eigen(R)$vectors[,1])
ranks <- obsname[order(score)]
track <- as.matrix(read.csv("Track_Records.csv", header = T))
track
ranks <- track$Country[order(score)]
track$Country
ranks <- track$Country[order(score),]
track.1 <- as.matrix(read.csv("Track_Records.csv", header = T))
ranks <- track.1$Country[order(score),]
score <- as.matrix(scale(track)) %*% as.matrix(eigen(R)$vectors[,1])
eigen(R)$vectors[,1] * sqrt(eigen(R)$values[1])
scale(track)
score <- as.matrix(track.new) %*% as.matrix(eigen(R)$vectors[,1])
ranks <- track.1$Country[order(score)]
track.1 <- as.matrix(read.csv("Track_Records.csv", header = T))
ranks <- track.1$Country[order(score)]
score
track.1$Country
track.1 <- as.matrix(read.csv("Track_Records.csv", header = T))
track.1
ranks <- track.1[order(score)]
ranks
score <- abs(as.matrix(track.new) %*% as.matrix(eigen(R)$vectors[,1]))
ranks <- track.1[order(score)]
ranks <- track.1[order(score)]; ranks
score
eigen(R)$vectors[,1])
eigen(R)$vectors[,1]
score <- abs(as.matrix(track.new) %*% as.matrix(eigen(R)$vectors[,1]))
score
score <- as.matrix(track.new) %*% as.matrix(eigen(R)$vectors[,1])
score
eigen(R)$vectors[,1]
score <- as.matrix(track.new) %*% as.matrix(abs(eigen(R)$vectors[,1]))
score
track.new
track.new <- scale(track, center=TRUE, scale=TRUE)
track.new <- scale(track, center=TRUE, scale=TRUE)
track <- as.matrix(read.csv("Track_Records.csv", header = T))
track.new <- scale(track, center=TRUE, scale=TRUE)
track <- as.matrix(read.csv("Track_Records.csv", header = T))
dim(track)
track <- matrix(as.numeric(track[,-1]),54,7)
R <- cor(track)
eigen(R)$values  # eigenvalues and eigenvetors
eigen(R)$vectors
prin_comp=prcomp(track.new, scale. = T)
track.new <- scale(track, center=TRUE, scale=TRUE)
score <- as.matrix(track.new) %*% as.matrix(eigen(R)$vectors[,1])
ranks <- track.1[order(score)]; ranks
score
ranks <- track.1[order(score),]; ranks
ranks <- track.1[order(score),]; ranks
ranks
score
ranks <- track.1[order(score)]; ranks
ranks <- track.1[order(score), decreasing=T]; ranks
score
order(score)
ranks <- track.1[order(score, decreasing = T)]; ranks
ranks <- track.1[order(score, decreasing = T)]; ranks
ranks
score
eigen(R)$vectors
as.matrix(track.new)
setwd("/Users/whs/Documents/GitHub/Spring2018-Project3-Group4")
y <- read.csv("./data/train/train/label_train.csv", header=T)
setwd("/Users/whs/Documents/GitHub/Spring2018-Project3-Group4")
y <- read.csv("./data/train/train/label_train.csv", header=T)
y <- read.csv("./data/train/label_train.csv", header=T)
train_proportion=0.70
# import training images labels
setwd("/Users/whs/Documents/GitHub/Spring2018-Project3-Group4")
y <- read.csv("./data/train/label_train.csv", header=T)
Y <- y[ ,-1]
Y <- as.factor(Y$label)
n <-length(Y)
# extract features
X <-read.csv("/Users/whs/Documents/GitHub/Spring2018-Project3-Group4/output/rgbftr.csv",header = T)
X <- X[, -1442]
X <- X[, -1] ## last col is label
head(X)
# split into training and testing dataset
set.seed(0)
Index <-sample(3000,round(train_proportion*3000,1),replace = F)
test  <- setdiff(1:3000, Index)
train.X <- data.matrix(X[Index,])
test.X <- data.matrix(X[test,])
train.Y <- Y[Index]
test.Y <- Y[-Index]
y <- read.csv("./data/train/label_train.csv", header=T)
y <- read.csv("../data/train/label_train.csv", header=T)
X <-read.csv("../output/rgbftr.csv",header = T)
seq(0.001, 0.1, by = 0.005)
seq(0.0001, 0.1, by = 0.05)
seq(0.0001, 0.1, by = 0.0055)
seq(0.0001, 0.1, by = 0.0005)
seq(0.0001, 0.1, by = 0.005)
seq(0.001, 0.1, by = 0.005)
y <- read.csv("../data/train/label_train.csv", header=T)
Y <- y[ ,-1]
Y <- as.factor(Y$label)
n <-length(Y)
X <-read.csv("../output/rgbftr.csv",header = T)
X <-read.csv("../output/rgbftr.csv",header = T)
X <- X[, -1442]
X <- X[, -1442]
X <- X[, -1]
Index <-sample(3000,round(train_proportion*3000,1),replace = F)
test  <- setdiff(1:3000, Index)
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
train_svm <- function(train.X, train.Y, cost){
model = svm(train.X, train.Y, cost = cost, kernel = "linear")
return(model)
}
train_svm_kernel <- function(train.X, train.Y, cost, gamma){
model = svm(train.X, train.Y, cost = cost, gamma = gamma, type = "C", kernel = "radial")
return(model)
}
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
svm.margin.cv <- function(dat.train, class.train, cost){
val.err.cost.interm = numeric(5)
val.err.cost.f = numeric(length(cost))
folds = cut(seq(1,nrow(dat.train)),breaks=5,labels=FALSE)
#Perform 5 fold cross validation
for(j in 1 :length(cost))
{
#j = 1
for(i in 1:5){
val.Indexes <- which(folds==i,arr.ind=TRUE)
val.Data <- dat.train[val.Indexes, ]
train.Data <- dat.train[-val.Indexes, ]
train.class = class.train[-val.Indexes]
val.class = class.train[val.Indexes]
#Train the model
model = svm(x = train.Data, y = as.factor(train.class), cost = cost[j], kernel = "linear")
#Prediction on the validation data
pred <- predict(model,val.Data)
#validation error for current iteration with current cost
val.err.cost.interm[i] = mean(pred != val.class)
}
#Obtain the validation error for the current cost
val.err.cost.f[j] = mean(val.err.cost.interm)
}
linear.cost = cost[which.min(val.err.cost.f)]
return(list(linear.cost, min(val.err.cost.f)))
}
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
set.seed(0)
train_proportion=0.70
y <- read.csv("../data/train/label_train.csv", header=T)
Y <- y[ ,-1]
Y <- as.factor(Y$label)
n <-length(Y)
X <-read.csv("../output/rgbftr.csv",header = T)
X <-read.csv("../output/rgbftr.csv",header = T)
X <- X[, -1442]
X <- X[, -1]
Index <-sample(3000,round(train_proportion*3000,1),replace = F)
test  <- setdiff(1:3000, Index)
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
train_svm <- function(train.X, train.Y, cost){
model = svm(train.X, train.Y, cost = cost, kernel = "linear")
return(model)
}
train_svm_kernel <- function(train.X, train.Y, cost, gamma){
model = svm(train.X, train.Y, cost = cost, gamma = gamma, type = "C", kernel = "radial")
return(model)
}
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
svm.kernel.cv(train.X, train.Y, c(0.001, 0.01, 0.03, 0.1), c(0.1, 0.2, 0.005))
svm.margin.cv <- function(dat.train, class.train, cost){
val.err.cost.interm = numeric(5)
val.err.cost.f = numeric(length(cost))
folds = cut(seq(1,nrow(dat.train)),breaks=5,labels=FALSE)
#Perform 5 fold cross validation
for(j in 1 :length(cost))
{
#j = 1
for(i in 1:5){
val.Indexes <- which(folds==i,arr.ind=TRUE)
val.Data <- dat.train[val.Indexes, ]
train.Data <- dat.train[-val.Indexes, ]
train.class = class.train[-val.Indexes]
val.class = class.train[val.Indexes]
#Train the model
model = svm(x = train.Data, y = as.factor(train.class), cost = cost[j], kernel = "linear")
#Prediction on the validation data
pred <- predict(model,val.Data)
#validation error for current iteration with current cost
val.err.cost.interm[i] = mean(pred != val.class)
}
#Obtain the validation error for the current cost
val.err.cost.f[j] = mean(val.err.cost.interm)
}
linear.cost = cost[which.min(val.err.cost.f)]
return(list(linear.cost, min(val.err.cost.f)))
}
svm.kernel.cv <- function(dat.train, class.train, cost, gamma){
folds = cut(seq(1,nrow(dat.train)),breaks=5,labels=FALSE)
val.par.frame = data.frame(cost = as.vector(mapply(rep,cost,length(gamma))), gamma = rep(gamma,length(cost)), error = NA)
val.err.i = c()
for(i in 1:nrow(val.par.frame))
{
#i = 1
for(j in 1:5)
{
val.Index = which(folds == j, arr.ind = TRUE)
val.data = dat.train[val.Index,]
train.data = dat.train[-val.Index,]
val.class = class.train[val.Index]
train.class =  class.train[-val.Index]
#Train SVM model with current cost and current gamma at the ith iteration
#model = Train.SVM.kernel(X=train.data.m.k,Y=train.class.m.k,cost=val.par.frame$cost[i],gamma = val.par.frame$gamma[i])
model = svm(x=train.data,y=as.factor(train.class),cost = val.par.frame$cost[i],gamma = val.par.frame$gamma[i],type = "C",kernel = "radial")
#Prediction on validation data with current cost and current gamma at current iteration
pred = predict(model,val.data)
#Validaiton error at this iteration with current gamma and cost
val.err.i[j] = mean(pred != val.class)
}
val.par.frame$error[i] = mean(val.err.i)
}
#For cost
kernel.cost = val.par.frame$cost[which.min(val.par.frame$error)]
#For gamma:
kernel.gamma = as.numeric(as.character(val.par.frame$gamma[which.min(val.par.frame$error)]))
return(list(cost = kernel.cost, gamma = kernel.gamma, cv.error = min(val.par.frame$error), frame = val.par.frame))
}
svm.margin.cv(train.X, train.Y, seq(0.001, 0.1, by = 0.005))
svm.kernel.cv(train.X, train.Y, c(0.001, 0.01, 0.1), c(0.1, 0.2, 0.005))
test_svm <- function(model, data, class){
pred <- predict(model,data)
return(mean(pred != class))
}
error.linear <- test_svm(train_svm(test.X, test.Y, 0.1), test.X, test.Y)
seq(0.001, 0.1, by = 0.005)
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005))
svm.kernel.cv(train.X, train.Y, c(0.001, 0.01, 0.1), c(0.1, 0.2, 0.005))
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
train_svm <- function(train.X, train.Y, cost){
model = svm(train.X, train.Y, cost = cost, kernel = "linear")
return(model)
}
train_svm_kernel <- function(train.X, train.Y, cost, gamma){
model = svm(train.X, train.Y, cost = cost, gamma = gamma, type = "C", kernel = "radial")
return(model)
}
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
svm.margin.cv <- function(dat.train, class.train, cost){
val.err.cost.interm = numeric(5)
val.err.cost.f = numeric(length(cost))
folds = cut(seq(1,nrow(dat.train)),breaks=5,labels=FALSE)
#Perform 5 fold cross validation
for(j in 1 :length(cost))
{
#j = 1
for(i in 1:5){
val.Indexes <- which(folds==i,arr.ind=TRUE)
val.Data <- dat.train[val.Indexes, ]
train.Data <- dat.train[-val.Indexes, ]
train.class = class.train[-val.Indexes]
val.class = class.train[val.Indexes]
#Train the model
model = svm(x = train.Data, y = as.factor(train.class), cost = cost[j], kernel = "linear")
#Prediction on the validation data
pred <- predict(model,val.Data)
#validation error for current iteration with current cost
val.err.cost.interm[i] = mean(pred != val.class)
}
#Obtain the validation error for the current cost
val.err.cost.f[j] = mean(val.err.cost.interm)
}
linear.cost = cost[which.min(val.err.cost.f)]
return(list(linear.cost, min(val.err.cost.f)))
}
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
c(seq(0.001, 0.1, by = 0.005))
source("../lib/svm.cv.R")
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
train.X
train.Y
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
source("../lib/svm.cv.R")
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
library(e1071)
library(EBImage)
svm.margin.cv(train.X, train.Y, c(seq(0.001, 0.1, by = 0.005)))
linear_params <- list(cost = seq(0.001, 0.1, by = 0.005))
linear_params <- list(cost = seq(0.001, 0.1, by = 0.005))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
library(e1071)
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
linear_params <- list(cost = seq(0.001, 0.1, by = 0.005))
lin_tc <- tune.control(cross = K)
K <- 5  # number of CV folds
linear_params <- list(cost = seq(0.001, 0.1, by = 0.005))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances
perf_linsvm
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
seq(0.001, 0.1, by = 0.005
seq(0.001, 0.1, by = 0.005)
seq(0.001, 0.1, by = 0.005)
linear_params <- list(cost = seq(0.001, 0.1, by = 0.005))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances
perf_linsvm
linear_params <- list(cost = c(0.001, 0.01, 0.1, 1, 10))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances
perf_linsvm
linear_params <- list(cost = c(0.00001, 0.0001, 0.001, 0.01, 0.1))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_summary <- summary(linsvm_tune)
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances
perf_linsvm
linear_params <- list(cost = c(0.0001, 0.001, 0.01, 0.1))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_summary <- summary(linsvm_tune)
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances
perf_linsvm <- linsvm_tune$performances; perf_linsvm
rbf_params <-list(cost=seq(1,10,by = 2),gamma=c(0.001,0.01,0.1,0.5,1))
rbf_tc <- tune.control(cross = K)
rbfsvm_tune <- tune(svm, train.x = train_x, train.y = train_y,
kernel = "radial", scale = F, ranges = rbf_params,
tunecontrol = rbf_tc)
rbfsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "radial", scale = F, ranges = rbf_params,
tunecontrol = rbf_tc)
rbfsvm_summary <- summary(rbfsvm_tune)
perf_rbfsvm <- rbfsvm_tune$performances
perf_rbfsvm
rbf_params <-list(cost=seq(1,10,by = 2),gamma=c(0.01, 0.1, 1, 10, 100))
rbf_tc <- tune.control(cross = K)
rbfsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "radial", scale = F, ranges = rbf_params,
tunecontrol = rbf_tc)
rbf_params <-list(cost=c(0.001, 0.01, 0.1, 1),gamma=c(0.01, 0.1, 1, 10, 100))
rbf_tc <- tune.control(cross = K)
rbfsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "radial", scale = F, ranges = rbf_params,
tunecontrol = rbf_tc)
rbfsvm_summary <- summary(rbfsvm_tune)
perf_rbfsvm <- rbfsvm_tune$performances
perf_rbfsvm
linsvm_best <- linsvm_summary$best.model
linsvm_pred <- predict(linsvm_best, test.X)
mean(linsvm_pred != test.Y)
rbfsvm_best <- rbfsvm_summary$best.model
rbfsvm_pred <- predict(rbfsvm_best, test.X)
mean(rbfsvm_pred != test.Y)
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
linsvm_best <- linsvm_summary$best.model
linsvm_pred <- predict(linsvm_best, test.X)
mean(linsvm_pred != test.Y)
linsvm_best
rbfsvm_best
rbfsvm_best <- rbfsvm_summary$best.model
rbfsvm_pred <- predict(rbfsvm_best, test.X)
mean(rbfsvm_pred != test.Y)
rbfsvm_best
rbfsvm_pred <- predict(rbfsvm_best, test.X)
mean(rbfsvm_pred != test.Y)
rbfsvm_best <- rbfsvm_summary$best.model
rbfsvm_pred <- predict(rbfsvm_best, test.X)
nrow(test.X)
ncol(test.X)
X <- X[, -1442]
X <- X[, -1]
y <- read.csv("../data/train/label_train.csv", header=T)
Y <- y[ ,-1]
Y <- as.factor(Y$label)
n <-length(Y)
X <-read.csv("../output/rgbftr.csv",header = T)
X <- X[, -1442]
X <-read.csv("../output/rgbftr.csv",header = T)
X <-read.csv("../output/rgbftr.csv",header = T)
X <- X[, -1]
X <- X[, -ncol(X)]
X[1,]
set.seed(0)
train_proportion=0.70
# import training images labels
y <- read.csv("../data/train/label_train.csv", header=T)
Y <- y[ ,-1]
Y <- as.factor(Y$label)
n <-length(Y)
# extract features
X <-read.csv("../output/rgbftr.csv",header = T)
X <- X[, -1]
X <- X[, -ncol(X)]
Index <-sample(3000,round(train_proportion*3000,1),replace = F)
test  <- setdiff(1:3000, Index)
train.X <- data.matrix(X[Index,]); test.X <- data.matrix(X[test,])
train.Y <- Y[Index]; test.Y <- Y[-Index]
# linear SVM with soft margin
linear_params <- list(cost = c(0.0001, 0.001, 0.01, 0.1))
lin_tc <- tune.control(cross = K)
linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "linear", scale = F, ranges = linear_params,
tunecontrol = lin_tc)
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances; perf_linsvm
# SVM with soft margin and RBF kernel
rbf_params <-list(cost=c(0.001, 0.01, 0.1, 1),gamma=c(0.01, 0.1, 1, 10, 100))
rbf_tc <- tune.control(cross = K)
rbfsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
kernel = "radial", scale = F, ranges = rbf_params,
tunecontrol = rbf_tc)
rbfsvm_summary <- summary(rbfsvm_tune)
perf_rbfsvm <- rbfsvm_tune$performances
# Linear SVM test set estimates of the error rates
linsvm_best <- linsvm_summary$best.model
linsvm_pred <- predict(linsvm_best, test.X)
mean(linsvm_pred != test.Y)
# RBF SVM test set estimates of the error rates
rbfsvm_best <- rbfsvm_summary$best.model
rbfsvm_pred <- predict(rbfsvm_best, test.X)
mean(rbfsvm_pred != test.Y)
mean(rbfsvm_pred != test.Y)
linsvm_best <- linsvm_summary$best.model
linsvm_pred <- predict(linsvm_best, test.X)
mean(linsvm_pred != test.Y)
# RBF SVM test set estimates of the error rates
rbfsvm_best <- rbfsvm_summary$best.model
rbfsvm_pred <- predict(rbfsvm_best, test.X)
mean(rbfsvm_pred != test.Y)
