---
title: "main_proj3"
author: "Group 4"
date: "March 20, 2018"
output: html_document
---

```{r}
packages.used <- c("readr", "ggplot2", "caret", "Matrix",
                   "xgboost","EBImage", "e1071","knitr")

# check packages that need to be installed.
packages.needed <- setdiff(packages.used,
                           intersect(installed.packages()[,1],
                                     packages.used))
# install additional packages
if(length(packages.needed) > 0) {
  install.packages(packages.needed,dependencies = TRUE,
  repos = 'http://cran.us.r-project.org')
}

library(readr)
library(ggplot2)
library(caret)
library(Matrix)
library(xgboost)
library(EBImage)
library(e1071)
library(knitr)
```

### Step 0: Specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir, eval=FALSE}
#setwd("") 
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
```

Provide directories for raw images. Training set and test set should be in different subfolders. 
```{r}
experiment_dir <- "../data/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
```

### Step 1: Set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```

Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications.

```{r model_setup}
#model_values <- seq(3, 11, 2)
#model_labels = paste("GBM with depth =", model_values)
```

### Step 2: Import training images class labels.

In the dataset, label 1,2 and 3 correspond to fried chickens, dogs and blueberry muffins.

```{r train_label}
label_train <- read.csv(paste(experiment_dir, "train/label_train.csv", sep=""), header=T)$label
```

### Step 3: Construct visual feature.

```{r feature}
source("../lib/feature.R")

set.seed(3)

time_ftrRGB <- system.time(rgb_feature <- featureRGB(img_train_dir,export = T))
cat("Time for constructing RGB features is",time_ftrRGB[3],"s \n")

rgb_feature$label <- label_train

trainimg <- sample(1:3000,2100)
testimg <- setdiff(1:3000,trainimg)
labeldf <- read.csv(paste(experiment_dir, "train/label_train.csv", sep=""), header=T)

img <- 1:3000
labeldf$train <- ifelse(img %in% trainimg,1,0)
write.csv(labeldf[,-1],file = "../data/train/label2.csv")

train.rgb <- rgb_feature[trainimg,]
test.rgb <- rgb_feature[testimg,]

write.csv(train.rgb,file = "../output/rgbftr_train.csv")
write.csv(test.rgb,file = "../output/rgbftr_test.csv")
write.csv(rgb_feature, file = "../output/rgbftr.csv")

#train.rgb <- read.csv("../output/rgbftr_train.csv")[,-1]
#test.rgb <- read.csv("../output/rgbftr_test.csv")[,-1]
```


### Step 4: Train a classification model with training images.

```{r}
source("../lib/train.R")
source("../lib/test.R")
```

## Baseline model: GBM
```{python}
 
import numpy as np
import os
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
label_path=os.path.expanduser("..\\data\\train\label_train.csv") # label
label=pd.read_csv(label_path)
siftpath=os.path.expanduser("..\\data\\train\\SIFT_train.csv")
sift_feature=pd.read_csv(siftpath,header=None)# load data
sift_feature.insert(loc=0, column='label', value=label.label)
def GetCvScore(model,data,k=5):
    seqindx=np.repeat(range(k),data.shape[0]/k) # generate a sequence
    np.random.shuffle(seqindx) # shuffle the sequence
    cvscore=[]
    for i in range(k):
        X=data.iloc[seqindx!=i,2:]
        y=data.iloc[seqindx!=i,0]
        clf = model
        clf.fit(X, y)
        thisscore=clf.score(data.iloc[seqindx==i,2:],data.iloc[seqindx==i,0])
        print("the score for the fold {} is {}".format(i,thisscore))
        cvscore.append(thisscore)
    print("total score for the fold is {}".format(sum(cvscore)/k))
GetCvScore(GradientBoostingClassifier(),data=sift_feature)  # gbm 


```


## Alternative model 1: Xgboost

In parameter selecting part, we select eta from 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, max depth from 3 to 8 by 5-fold cross validation. 

```{r}
time_cv.Xgb <- system.time(cv_rgb <- xgb_param(train.rgb,K))
cat("Time for selecting best parameters is",time_cv.Xgb[3],"s \n")

param <- cv_rgb$best_param
#param <- list(eta = 0.15, max_depth = 4)

time_model1 <- system.time(model <- xgb_model(train.rgb,param))
cat("Time for building xgboost model is",time_model1[3],"s \n")

time_pred1 <- system.time(pred <- xgb_pred(model, test.rgb))
cat("Time for predicting test data is",time_pred1[3],"s \n") 
err1 <- pred$err
err1
```


## Alternative model 2: SVM

In parameter selecting part, for linear SVM, we select cost from 0.0001, 0.001, 0.01 and 0.1. For RBF kernal, we select cost from 0.0001, 0.001, 0.01 and 0.1, and selet gamma from 0.01, 0.1, 1, 10 and 100.


```{r}
train.X <- train.rgb[,-ncol(train.rgb)]
train.Y <- as.factor(train.rgb$label)

test.X <- test.rgb[,-ncol(test.rgb)]
test.Y <- as.factor(test.rgb$label)
# linear SVM with soft margin
linear_params <- list(cost = c(0.0001, 0.001, 0.01, 0.1))
lin_tc <- tune.control(cross = K)
time_cv.linsvm <- system.time(linsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
                    kernel = "linear", scale = F, ranges = linear_params, 
                    tunecontrol = lin_tc))
linsvm_summary <- summary(linsvm_tune)
perf_linsvm <- linsvm_tune$performances; perf_linsvm 

# SVM with soft margin and RBF kernel
rbf_params <-list(cost=c(0.001, 0.01, 0.1, 1),gamma=c(0.01, 0.1, 1, 10, 100))

rbf_tc <- tune.control(cross = K)
time_cv.RBFsvm <- system.time(rbfsvm_tune <- tune(svm, train.x = train.X, train.y = train.Y,
               kernel = "radial", scale = F, ranges = rbf_params, 
               tunecontrol = rbf_tc))
rbfsvm_summary <- summary(rbfsvm_tune)
perf_rbfsvm <- rbfsvm_tune$performances

# Linear SVM test set estimates of the error rates
time_model2 <- system.time(linsvm_best <- linsvm_summary$best.model)
time_pred2 <- system.time(linsvm_pred <- predict(linsvm_best, test.X))
err2 <- mean(linsvm_pred != test.Y)
err2

# RBF SVM test set estimates of the error rates
time_model3 <- system.time(rbfsvm_best <- rbfsvm_summary$best.model)
time_pred3 <- system.time(rbfsvm_pred <- predict(rbfsvm_best, test.X))
err3 <- mean(rbfsvm_pred != test.Y)
err3
```

### Summary of performances
```{r}
showtable <- data.frame("Error"=c(err1,err2,err3),
                        "CV time" = c(time_cv.Xgb[3], time_cv.linsvm[3],time_cv.RBFsvm[3]),
                        "Model training time"=c(time_model1[3],time_model2[3],time_model3[3]),
                        "Prediction time"=c(time_pred1[3],time_pred2[3],time_pred3[3]))

kable(showtable, caption = "Performance Report")
```


### Final Advanced Model: Fine tuning on MobileNet
This method is built on python 3.6, required packages: keras, tensorflow 1.4, opencv-python, skimage, pickle, scikit-learn.
### Preprocessing
resize the images to a fixed size.
```{r}
system('python ../lib/preprocessing.py --img_size=224')
```

### Corss Validation
Conduct cross validation on the number of hidden unit in the last hidden dense layer.
```{r}
if(run.cv){
  system('python ../lib/cross_validation.py --k 5 --hidden_unit_list 256 512 1024 --batch_size 128')
}
```
Running this on GCP with Nvidia K80, need 2 hours (about 20s an epoch). The result validation scores are [0.99095238095238103, 0.98857142857142866, 0.98999999999999999] for 256 512 1024 units. This would indicate that the hidden unit size might have minor effect on the result.

Here is the result copy from GCP
```{r}
cv_256 <- c(0.98809523809523814, 0.98809523809523814, 0.99523809523809526, 0.98809523809523814, 0.99523809523809526)
cv_512 <- c(0.99047619047619051, 0.98333333333333328, 0.99047619047619051, 0.98571428571428577, 0.99285714285714288)
cv_1024 <- c(0.98333333333333328, 0.98809523809523814, 0.99285714285714288, 0.99047619047619051, 0.99523809523809526)
library(ggplot2)
cv_data <- NULL
cv_data$y <- c(cv_256, cv_512, cv_1024)
cv_data$x <- as.factor(rep(c(256,512,1024),each=5))
cv_data <- as.data.frame(cv_data)
ggplot(data=cv_data,aes(x=x,y=y))+
  geom_boxplot(aes(fill=x))+
  xlab('number of units in the last dense layer')+
  ylab('5 folds validation accuracy')
```


### Training on partial of the provided training data (2100), predict on validation data (900) to compare with other models.
```{r}
system('python ../lib/train.py --hidden_unit 256 --epochs 30 --all_data 0')
```

### Training on all provided data and save the model
```{r}
system.time(system('python ../lib/train.py --hidden_unit 256 --epochs 30 --all_data 1'))
```

### Test new data
```{r}
# resize data
system.time(system('python ../lib/preprocessing.py --img_size=224 --train 0 --img_dir ../data/test/images --lab_dir ../data/test/label.csv'))
```

```{r}
if (run.test){
  system.time(system('python ../lib/test.py --hidden_unit 256 --model_path ../output/keras_model/mobilenet_1522197600.hdf5 '))
}
```
